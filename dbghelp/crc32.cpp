/* Generated by https://github.com/corsix/fast-crc32/ using: */
/* crc_generator.exe -i avx -p crc32c -a v3s2x4 */
/* AND THEN EDITED BY HAND TO SUCK LESS FOR 32 BIT BUILDS */
/* MIT licensed */

#include <stddef.h>
#include <stdint.h>
#include <nmmintrin.h>
#include <wmmintrin.h>

typedef uint64_t vec64u __attribute__((__vector_size__(16)));
typedef uint32_t vec32u __attribute__((__vector_size__(16)));

#if defined(_MSC_VER)
#define CRC_AINLINE static __forceinline
#define CRC_ALIGN(n) __declspec(align(n))
#define CRC_NINLINE __declspec(noinline)
#else
#define CRC_AINLINE static __inline __attribute__((always_inline))
#define CRC_ALIGN(n) __attribute__((aligned(n)))
#define CRC_NINLINE __attribute__((never_inline))
#endif

#define clmul_lo(a, b) (_mm_clmulepi64_si128((a), (b), 0))
#define clmul_hi(a, b) (_mm_clmulepi64_si128((a), (b), 17))

#if !__x86_64__
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u64(unsigned long long __C, unsigned long long __D) {
    return _mm_crc32_u32(_mm_crc32_u32(__C, __D), __D >> 32);
}
#endif

CRC_AINLINE __m128i clmul_scalar(uint32_t a, uint32_t b) {
    return _mm_clmulepi64_si128(_mm_cvtsi32_si128(a), _mm_cvtsi32_si128(b), 0);
}

static uint32_t xnmodp(uint64_t n) /* x^n mod P, in log(n) time */ {
    uint64_t stack = ~(uint64_t)1;
    uint32_t acc;
    for (; n > 191; n = (n >> 1) - 16) {
        stack = (stack << 1) + (n & 1);
    }
    uint32_t n_low = n;
    stack = ~stack;
    acc = ((uint32_t)0x80000000) >> (n_low & 31);
    for (n_low >>= 5; n_low; --n_low) {
        acc = _mm_crc32_u32(acc, 0);
    }

    uint32_t stack_high = stack >> 32;
    int32_t stack_high_bits = 32 - __builtin_ia32_lzcnt_u32(stack_high);

    uint32_t stack_low = stack;
    int32_t stack_low_bits = 32;
    if (!stack_high_bits) {
        stack_low_bits = 32 - __builtin_ia32_lzcnt_u32(stack_low);
    }
    while (--stack_low_bits > 0) {
        uint32_t low = stack_low & 1;
        stack_low >>= 1;
        __m128i x = _mm_cvtsi32_si128(acc);
        uint64_t y = _mm_cvtsi128_si64(_mm_clmulepi64_si128(x, x, 0));
        acc = _mm_crc32_u64(0, y << low);
    }
    while (--stack_high_bits > 0) {
        uint32_t low = stack_high & 1;
        stack_high >>= 1;
        __m128i x = _mm_cvtsi32_si128(acc);
        uint64_t y = _mm_cvtsi128_si64(_mm_clmulepi64_si128(x, x, 0));
        acc = _mm_crc32_u64(0, y << low);
    }
    return acc;
}

CRC_AINLINE __m128i crc_shift(uint32_t crc, size_t nbytes) {
    return clmul_scalar(crc, xnmodp((uint64_t)nbytes * 8 - 33));
}

CRC_NINLINE uint32_t __fastcall crc32_impl(const uint8_t* buf, size_t len) {
    uint32_t crc0 = ~0;

    if (len >= 80) {
        size_t blk = len / 80;
        len %= 80;
        size_t klen = blk * 16;
        const uint8_t* buf2 = buf + klen * 2;
        uint32_t crc1 = 0;
        __m128i vc0;
        __m128i vc1;
        /* First vector chunk. */
        __m128i x0 = _mm_load_si128((const __m128i*)buf2), y0;
        __m128i x1 = _mm_load_si128((const __m128i*)(buf2 + 16)), y1;
        __m128i x2 = _mm_load_si128((const __m128i*)(buf2 + 32)), y2;
        __m128i k;
        k = _mm_setr_epi32(0x1c291d04, 0, 0xddc0152b, 0);
        buf2 += 48;

        const uint8_t* end = buf + klen;

        /* Main loop. */
        for (;;) {
            crc0 = _mm_crc32_u32(crc0, *(const uint32_t*)buf);
            crc1 = _mm_crc32_u32(crc1, *(const uint32_t*)(buf + klen));
            crc0 = _mm_crc32_u32(crc0, *(const uint32_t*)(buf + 4));
            crc1 = _mm_crc32_u32(crc1, *(const uint32_t*)(buf + klen + 4));
            crc0 = _mm_crc32_u32(crc0, *(const uint32_t*)(buf + 8));
            crc1 = _mm_crc32_u32(crc1, *(const uint32_t*)(buf + klen + 8));
            crc0 = _mm_crc32_u32(crc0, *(const uint32_t*)(buf + 12));
            crc1 = _mm_crc32_u32(crc1, *(const uint32_t*)(buf + klen + 12));
            buf += 16;
            __asm__("":"+r"(end));
            if (buf >= end) break;
            y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
            y1 = clmul_lo(x1, k), x1 = clmul_hi(x1, k);
            y2 = clmul_lo(x2, k), x2 = clmul_hi(x2, k);
            y0 = _mm_xor_si128(y0, _mm_load_si128((const __m128i*)buf2)), x0 = _mm_xor_si128(x0, y0);
            y1 = _mm_xor_si128(y1, _mm_load_si128((const __m128i*)(buf2 + 16))), x1 = _mm_xor_si128(x1, y1);
            y2 = _mm_xor_si128(y2, _mm_load_si128((const __m128i*)(buf2 + 32))), x2 = _mm_xor_si128(x2, y2);
            buf2 += 48;
            //len -= 80;
        }
        buf = buf2;
        /* Reduce x0 ... x2 to just x0. */
        k = _mm_setr_epi32(0xf20c0dfe, 0, 0x493c7d27, 0);
        y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
        y0 = _mm_xor_si128(y0, x1), x0 = _mm_xor_si128(x0, y0);
        x1 = x2;
        y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
        y0 = _mm_xor_si128(y0, x1), x0 = _mm_xor_si128(x0, y0);
        /* Final scalar chunk. */
        vc0 = crc_shift(crc0, klen + blk * 48);
        vc1 = crc_shift(crc1, blk * 48);
        vc0 = _mm_xor_si128(vc0, vc1);
        /* Reduce 128 bits to 32 bits, and multiply by x^32. */
        crc0 = _mm_crc32_u64(0, _mm_extract_epi64(x0, 0));
        vc0 = _mm_xor_si128(_mm_set_epi64x(_mm_extract_epi64(x0, 1), _mm_extract_epi64(x0, 1)), vc0);
        crc0 = _mm_crc32_u64(crc0, _mm_extract_epi64(vc0, 0));
    }
    for (; len >= 4; buf += 4, len -= 4) {
        crc0 = _mm_crc32_u32(crc0, *(const uint32_t*)buf);
    }
    for (; len; --len) {
        crc0 = _mm_crc32_u8(crc0, *buf++);
    }
    return ~crc0;
}
